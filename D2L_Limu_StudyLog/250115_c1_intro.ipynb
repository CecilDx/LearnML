{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d37f43-1544-43f5-ac1b-adbf7c677ffe",
   "metadata": {},
   "source": [
    "# 1. 总览\n",
    "## 1.1 何为业务逻辑\n",
    "- 业务逻辑说明了应用程序在不同情况下如何满足用户需求\n",
    "- 为了完善业务逻辑，需要细致地考虑应用程序可能遇到的边界情况，为边界情况设定规则\n",
    "- 例如：当买家单击将商品添加到购物车时，应用程序会向购物车数据库表中添加一个条目，将该用户 ID 与商品 ID 关联起来\n",
    "## 1.2 何为模型\n",
    "- 模型是调整参数后的程序\n",
    "- 基于数据集调整参数的元程序被称为学习算法\n",
    "## 1.3 ML 的组件\n",
    "- data，用于学习\n",
    "- model，用于转化数据\n",
    "- objective function，用于量化 model 有效性\n",
    "- algorithm，用于调整参数优化目标函数\n",
    "## 1.4 data 相关\n",
    "- 独立同分布，Independent and Identically Distributed，i.i.d.，指一组随机变量，这些变量满足互相独立且均满足相同的分布。i.i.d. 使计算更便捷，许多统计推断和机器学习依赖于该假设\n",
    "- 协变量 / 特征（features / covariates），通常指模型中的输入变量或特征变量，用于预测目标变量（因变量，标签 / 目标，label / target）。在回归中，协变量是自变量，分类中即特征\n",
    "- 数据的维度，dimensionality，特征类别数量 => 特征向量\n",
    "## 1.5 objective function 目标函数相关\n",
    "- 目标函数用于定义模型的优劣程度的度量，目标函数可被优化。一般而言，优化即将目标函数优化至最低点，因此也成为损失函数（loss function / cost function）\n",
    "- 回归中最常见的 loss 为 squared error 平方误差，即预测值和实际值之差的平方，分类中常见的 loss 为最小化错误率，即预测与实际情况不符的样本比例。由于不同的 loss function 的优化难度不同，有些可通过替代目标优化，即换一个可微且相关的 loss\n",
    "## 1.6 algorithm 优化算法相关\n",
    "- 当有了 data、model 和 loss 后，需要一个 algorithm 进行最佳参数的搜索从而最小化 loss\n",
    "- DL 中，大多数 algorithm 通常基于 gradient descent 梯度下降方法\n",
    "- GD 即在每个步骤中，GD algorithm 检查每个参数，判断变动该参数后，loss 是增大还是缩小，然后在缩小的方向上优化参数\n",
    "## 1.7 ML 类别\n",
    "- 监督学习：学习已知 feature - label 对（样本 example），预测给定 feature 的 label，用概率论术语来说，即预测“估计给定输入特征的标签”的条件概率。学习过程：选择样本子集（已知 feature - label 或者人工标记 label）构成训练集 - 选择有监督学习算法输入训练集输出完成学习的模型 - 输入未知样本预测 label\n",
    "- 监督学习包括：\n",
    "    - 回归：预测一个数值，常见 loss 为平方误差损失函数\n",
    "    - 分类：预测一个类别，比如某个字符、数字等，包括二项分类、多项分类和层次分类等，常见 loss 为交叉熵，cross-entropy\n",
    "    - 标记：描绘输入的内容，比如一张图片有哪些植物动物，如多标签分类等，可用于标记\n",
    "    - 搜索：在海量搜索结果中找到用户最需要的部分，并按照相关性排序，当下搜索引擎使用 ML 和用户行为模型获取网页相关性得分\n",
    "    - 推荐系统：推荐系统为给定用户和物品的匹配性打分，该分数可能是估计的评级或购买的概率，并将得分最高的对象集推荐给给定用户。系统训练集来源于用户反馈\n",
    "    - 序列学习：不同于以上大部分问题（固定大小输入 - 固定大小输出），序列学习解决连续输入背景下，model 的记忆功能，比如处理视频片段的每一帧需要参考之前的帧，处理文字序列等。序列学习的应用场景包括（不限于）：\n",
    "        - 标记和解析：用属性注释文本序列，比如预测某个文本序列中哪些词是动词，需要 model 基于结构和语法对文本做分解和注释\n",
    "        - 自动语音识别：音频 - 文本\n",
    "        - 文本到语音\n",
    "        - 机器翻译\n",
    "- 无监督学习\n",
    "    - 聚类：clustering 无标签情况下进行分类\n",
    "    - 主成分分析：principal component analysis 找到少量参数准确捕捉数据的线性相关属性\n",
    "    - 因果关系和概率图模型：causality and probabilistic graphical models，观察数据的根本原因\n",
    "    - 生成对抗网络：generative adversarial networks，合成数据\n",
    "- 离线学习：offline learning，先获取数据再启动模型\n",
    "- 强化学习：reinforcement learning，智能体 agent 在时间序列上与环境交互，agent 从环境获得观察 observation，选择一个动作 action，通过执行器传输回环境，最终 agent 从环境获得奖励 reward，再进行新循环。RL 目标是一个好的策略 policy，action 受 policy 控制，即 How obeservation => action\n",
    "    - RL - 有监督：如分类 => action, 创建环境 action => reward，reward = loss\n",
    "    - 考虑环境的可观测性\n",
    "    - 在任何时间点上，强化学习智能体可能知道一个好的策略，但可能有许多更好的策略从未尝试过的。 强化学习智能体必须不断地做出选择：是应该利用当前最好的策略，还是探索新的策略空间\n",
    "    - 当环境可被完全观察到，RL => 马尔可夫决策过程 markov decision process；反之，当状态不依赖之前的操作，RL => 上下文赌博机 contextual bandit problem；当没有状态，只有一组最初未知回报的可用动作，RL => 多臂赌博机 multi-armed bandit problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4502be-898c-4ebd-9e35-cb2aeb889a74",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe38a6-ec78-4f93-90b2-cc2aa8922297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
